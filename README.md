# LLM Serving API with Chat Interface

## Overview
A production-ready LLM serving API built with FastAPI that supports both GPT-2 and Llama3 models with an interactive chat interface. Features include conversational AI, model switching, conversation management, and a rich command-line chat experience.

## Features
- ðŸ¤– **Multiple Models**: GPT-2, GPT-2 Medium, Llama3 1B, Llama3 3B, DistilGPT2
- ðŸ’¬ **Chat Interface**: Interactive CLI with rich formatting and commands
- ðŸ”„ **Model Switching**: Switch between models during conversations
- ðŸ’¾ **Conversation Management**: Save and load chat histories
- ðŸš€ **Fast API**: Async FastAPI server with comprehensive REST endpoints
- ðŸ§  **Context Awareness**: Maintains conversation history and context
- ðŸŽ¨ **Rich CLI**: Beautiful terminal interface with colors and formatting

## Technical Stack
- **Framework**: FastAPI (async API server)
- **Models**: Hugging Face Transformers (GPT-2, Meta Llama3)
- **Chat System**: Custom conversation management with context
- **CLI**: Rich + prompt-toolkit for interactive experience
- **Runtime**: PyTorch with CPU/GPU support
- **Validation**: Pydantic schemas and request validation

## Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Start the API Server
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 3. Start Chat Interface
```bash
python chat_cli.py
```

### 4. Chat Commands
```bash
# List available models
/models

# Switch to Llama3 model
/switch llama3-1b

# Start chatting
Hello, how are you today?

# Save conversation
/save my_conversation

# Load previous conversation
/load my_conversation

# Get help
/help

# Exit
/quit
```

## Available Models

| Model | Size | Memory | Context | Best For |
|-------|------|---------|---------|----------|
| `gpt2` | 124MB | ~2GB | 1024 | Quick responses, lightweight |
| `gpt2-medium` | 355MB | ~4GB | 1024 | Better quality, moderate size |
| `llama3-1b` | 1.2GB | ~4GB | 8192 | Chat conversations, efficient |
| `llama3-3b` | 3.2GB | ~8GB | 8192 | High-quality chat, larger context |
| `distilgpt2` | 82MB | ~1GB | 1024 | Fastest, smallest model |

## Chat Interface Usage

### Basic Chat Example
```bash
$ python chat_cli.py

Welcome to LLM Chat Interface!
Current model: llama3-1b

You: Hello! Can you help me write a Python function?
Assistant: Of course! I'd be happy to help you write a Python function. 
What specific functionality would you like the function to have?

You: A function that calculates the factorial of a number
Assistant: Here's a Python function to calculate the factorial of a number:

```python
def factorial(n):
    if n < 0:
        raise ValueError("Factorial is not defined for negative numbers")
    elif n == 0 or n == 1:
        return 1
    else:
        result = 1
        for i in range(2, n + 1):
            result *= i
        return result
```

You: /save factorial_help
ðŸ’¾ Conversation saved to factorial_help.json

You: /quit
ðŸ‘‹ Goodbye!
```

### Model Switching Example
```bash
You: /models
ðŸ“‹ Available Models:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Model         â”ƒ Status   â”ƒ Description                                  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ llama3-1b     â”‚ âœ… Loaded â”‚ Meta Llama 3.2 1B Instruct                 â”‚
â”‚ llama3-3b     â”‚ Availableâ”‚ Meta Llama 3.2 3B Instruct                 â”‚
â”‚ gpt2          â”‚ Availableâ”‚ OpenAI GPT-2                                â”‚
â”‚ gpt2-medium   â”‚ Availableâ”‚ OpenAI GPT-2 Medium                         â”‚
â”‚ distilgpt2    â”‚ Availableâ”‚ DistilGPT2                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

You: /switch gpt2-medium
ðŸ”„ Switching to model: gpt2-medium
â³ Loading model... (this may take a moment)
âœ… Successfully switched to gpt2-medium

You: Hello again with the new model!
Assistant: Hello! I'm now running on GPT-2 Medium. How can I assist you today?
```

## API Endpoints

### Chat Endpoints
- `POST /api/v1/chat` - Send a chat message
- `POST /api/v1/chat/new` - Start a new conversation
- `GET /api/v1/chat/models` - List available models
- `POST /api/v1/chat/switch-model` - Switch to a different model
- `GET /api/v1/chat/conversation/{id}` - Get conversation history

### Legacy Generation Endpoints
- `POST /api/v1/generate` - Generate text completion
- `GET /api/v1/health` - Health check endpoint
- `GET /api/v1/model-info` - Model information and status
- `POST /api/v1/load-model` - Load model explicitly

### API Usage Examples

#### Start a New Chat Conversation
```bash
curl -X POST "http://localhost:8000/api/v1/chat/new" \
  -H "Content-Type: application/json"
```

Response:
```json
{
  "conversation_id": "conv_abc123",
  "message": "New conversation started"
}
```

#### Send a Chat Message
```bash
curl -X POST "http://localhost:8000/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is the capital of France?",
    "conversation_id": "conv_abc123",
    "max_length": 100,
    "temperature": 0.7
  }'
```

Response:
```json
{
  "response": "The capital of France is Paris. It is located in the north-central part of the country.",
  "conversation_id": "conv_abc123",
  "generation_time": 1.23
}
```

#### Switch Models
```bash
curl -X POST "http://localhost:8000/api/v1/chat/switch-model" \
  -H "Content-Type: application/json" \
  -d '{"model_name": "llama3-1b"}'
```

## Configuration

Environment variables with `LLM_` prefix:

- `LLM_MODEL_NAME`: Default model (default: "llama3-1b")
- `LLM_DEVICE`: "cpu" or "cuda" (default: "cpu")  
- `LLM_MODEL_CACHE_DIR`: Model cache directory (default: "./models")
- `LLM_LOG_LEVEL`: Logging level (default: "INFO")
- `LLM_MAX_CONVERSATION_LENGTH`: Max conversation turns (default: 50)

Example `.env` file:
```bash
LLM_MODEL_NAME=llama3-3b
LLM_DEVICE=cuda
LLM_MODEL_CACHE_DIR=/path/to/models
LLM_LOG_LEVEL=DEBUG
```

## Testing

### Run API Tests
```bash
# Basic API tests
python client_test.py

# Interactive API testing
python client_test.py --interactive

# Chat functionality tests
pytest tests/test_chat.py -v

# All tests
pytest tests/ -v
```

### Standalone Model Testing
```bash
# Test default model
python standalone_test.py

# Test specific model
python standalone_test.py --model llama3-1b

# Interactive mode
python standalone_test.py --interactive


## Project Structure

```
llm_serving/
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ CLAUDE.md              # Development guidelines
â”œâ”€â”€ planning.md            # Implementation plan and status
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ chat_cli.py            # Interactive chat interface
â”œâ”€â”€ client_test.py         # API client tests
â”œâ”€â”€ standalone_test.py     # Standalone model tests
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py           # FastAPI application setup
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ endpoints.py  # API endpoint implementations
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py     # Configuration and model profiles
â”‚   â”‚   â””â”€â”€ logging.py    # Logging setup
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ chat_manager.py    # ChatModelManager for conversation AI
â”‚       â”œâ”€â”€ conversation.py    # Conversation and Message classes
â”‚       â”œâ”€â”€ model_manager.py   # Legacy ModelManager (GPT-2 only)
â”‚       â””â”€â”€ schemas.py         # Pydantic request/response models
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_api.py        # Basic API tests
    â””â”€â”€ test_chat.py       # Chat functionality tests
```

## Troubleshooting

### Model Authentication Issues

**ðŸ”’ "Access to model is restricted" Error**
Some models (like Meta Llama) require Hugging Face authentication:

```bash
# Option 1: Use pre-configured open models (recommended)
# The system is already configured with open alternatives:
# - llama3-1b â†’ DialoGPT Medium (no auth required)
# - llama3-3b â†’ GPT-2 Large (no auth required)

# Option 2: Set up Hugging Face authentication
pip install huggingface_hub
huggingface-cli login
# Then accept model license at: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
```

### Chat Behavior Issues

**ðŸ”„ Model Returns Same Message as Prompt**
When models echo your input instead of responding:

```bash
# Test with higher temperature
curl -X POST "http://localhost:8000/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hello!",
    "temperature": 0.9,
    "max_tokens": 100
  }'

# Or adjust in chat CLI
ðŸ’¬ You: /switch gpt2
ðŸ’¬ You: Hello, how are you?
```

**ðŸŽ² GPT-2 Returns Random Text**
GPT-2 needs proper conversation context:
- âœ… **Fixed**: GPT-2 now has improved chat templates
- Use `/switch gpt2` for better conversational responses
- If still having issues, try `/switch llama3-1b` (DialoGPT)

**ðŸ”§ Model Parameter Tuning**
Adjust generation parameters for better responses:

| Parameter | Low Value Issue | High Value Issue | Recommended |
|-----------|----------------|------------------|-------------|
| `temperature` | Repetitive, boring | Random, nonsensical | 0.7-0.9 |
| `max_tokens` | Cut-off responses | Too verbose | 50-150 |
| `top_p` | Limited variety | Too scattered | 0.8-0.95 |
| `top_k` | Repetitive | Unfocused | 40-60 |

### Chat Interface Issues

**âš¡ "asyncio.run() cannot be called from a running event loop"**
Running chat CLI from async environment (Jupyter, IPython):

```bash
# Solution 1: Install nest-asyncio
pip install nest-asyncio
python chat_cli.py

# Solution 2: Use regular terminal
# Open plain command prompt/terminal (not Jupyter)
python chat_cli.py

# Solution 3: Use API directly
curl -X POST "http://localhost:8000/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello!"}'

# Solution 4: Use web interface
# Open browser: http://localhost:8000/docs
```

### Model Performance Issues

**ðŸš« 503 Service Unavailable on Model Switch**
Model failed to load, usually due to memory:

```bash
# Check available memory
free -h  # Linux
vm_stat | grep "free\|inactive"  # macOS

# Solutions:
ðŸ’¬ You: /switch llama3-1b    # Try smaller model (2GB)
ðŸ’¬ You: /switch gpt2         # Even smaller (2GB)
ðŸ’¬ You: /switch distilgpt2   # Smallest (1GB)

# Or enable GPU if available
export LLM_DEVICE=cuda
```

**ðŸ“Š Model Memory Requirements**

| Model | Memory | Speed | Quality | Best For |
|-------|--------|-------|---------|----------|
| `distilgpt2` | ~1GB | âš¡âš¡âš¡ | â­â­ | Testing, low-resource |
| `gpt2` | ~2GB | âš¡âš¡ | â­â­â­ | âœ… **General chat** |
| `llama3-1b` | ~2GB | âš¡âš¡ | â­â­â­â­ | âœ… **Best conversation** |
| `llama3-3b` | ~3GB | âš¡ | â­â­â­â­â­ | High-quality chat |

### Common Issues

**ðŸ’¾ Memory Issues**
- Start with smaller models (`gpt2`, `distilgpt2`, `llama3-1b`)
- Use `LLM_DEVICE=cpu` if GPU memory is insufficient
- Monitor system memory usage during model loading
- Close other applications to free up RAM

**ðŸ“¥ Model Loading Errors**
- Check internet connection for initial model downloads
- Verify sufficient disk space in model cache directory (models can be 1-3GB)
- Clear model cache if corruption suspected: `rm -rf ./models/`
- Wait patiently - first download can take several minutes

**ðŸ” Permission Errors**
- Ensure write permissions for model cache directory
- Use absolute paths in environment variables
- On Linux/macOS: `chmod 755 ./models/`

**ðŸ–¥ï¸ CLI Issues**
- Install required packages: `pip install rich prompt-toolkit nest-asyncio`
- Use Python 3.8+ for best compatibility
- Check terminal supports UTF-8 for rich formatting
- Try different terminal if colors/formatting broken

### API Testing & Debugging

**ðŸ§ª Test API Endpoints**
```bash
# 1. Health check
curl http://localhost:8000/api/v1/health

# 2. List models
curl http://localhost:8000/api/v1/chat/models

# 3. Test chat
curl -X POST "http://localhost:8000/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello!"}'

# 4. Test with custom parameters
curl -X POST "http://localhost:8000/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Write a Python function to add two numbers",
    "temperature": 0.8,
    "max_tokens": 150
  }'
```

**ðŸ” Check Server Logs**
Monitor the uvicorn server output for detailed error messages:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
# Watch for error messages in the console output
```

### Getting Help

If you're still having issues:

1. **ðŸ“‹ Check server logs** for detailed error messages
2. **ðŸ§ª Test with API directly** using curl commands above
3. **ðŸ’¾ Try different models** starting with smallest (`distilgpt2`)
4. **ðŸ”„ Restart the server** after configuration changes
5. **ðŸŒ Use web interface** at `http://localhost:8000/docs` for testing

**ðŸŽ¯ Quick Diagnosis**
```bash
# Run this diagnostic sequence:
curl http://localhost:8000/api/v1/health && echo "âœ… Server OK" || echo "âŒ Server issue"
curl http://localhost:8000/api/v1/chat/models && echo "âœ… Models OK" || echo "âŒ Model config issue"
curl -X POST "http://localhost:8000/api/v1/chat" -H "Content-Type: application/json" -d '{"message":"test"}' && echo "âœ… Chat OK" || echo "âŒ Chat issue"
```

### Performance Tips

- **GPU Acceleration**: Set `LLM_DEVICE=cuda` if NVIDIA GPU available
- **Model Selection**: Choose model size based on available memory
- **Conversation Length**: Adjust `LLM_MAX_CONVERSATION_LENGTH` for memory optimization
- **Batch Processing**: Use API endpoints for multiple requests

## Development

### Adding New Models
1. Add model profile to `app/core/config.py`
2. Update `ChatModelManager` if special handling needed
3. Add tests in `tests/test_chat.py`
4. Update documentation

### Contributing
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Update documentation
5. Submit a pull request

## License

This project is provided as-is for educational and research purposes.

## Acknowledgments

- **Meta**: For the Llama3 models
- **OpenAI**: For GPT-2 models
- **Hugging Face**: For the transformers library
- **FastAPI**: For the excellent web framework
- **Rich**: For beautiful terminal formatting

---

*Happy chatting! ðŸ¤–âœ¨*
